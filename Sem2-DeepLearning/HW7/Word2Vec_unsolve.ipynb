{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bGq1XjUcwcWn"
      },
      "outputs": [],
      "source": [
        "# Word2Vec Implementation\n",
        "\n",
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [Errno 104] Connection\n",
            "[nltk_data]     reset by peer>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ihrIIAXNwgBA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the Word2Vec model class\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    def forward(self, target_word, context_word):\n",
        "        target_embed = self.in_embed(target_word)\n",
        "        context_embed = self.out_embed(context_word)\n",
        "        return target_embed, context_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DzHH19FowgEH"
      },
      "outputs": [],
      "source": [
        "# Define the training function\n",
        "\n",
        "def train_word2vec(corpus, window_size, embedding_dim, num_epochs, learning_rate):\n",
        "    # Preprocess the corpus and build the vocabulary\n",
        "    sentences = [sent.strip().replace('!', '') for sent in corpus.split('.')]\n",
        "    sentences = [f'<bos> {sent} <eos>' for sent in sentences]\n",
        "    \n",
        "    vocab = []\n",
        "    for sent in sentences:\n",
        "        vocab += word_tokenize(sent)\n",
        "    \n",
        "    vocab = list(set(vocab))\n",
        "    stoi = {v:k for k, v in enumerate(vocab)}\n",
        "    itos = {k:v for k, v in enumerate(vocab)}\n",
        "    \n",
        "    # Create the target-context word pairs\n",
        "    training_pairs = []\n",
        "    for sent in sentences:\n",
        "        sent_toks = word_tokenize(sent)\n",
        "        \n",
        "        for i in range(len(sent_toks)-window_size):\n",
        "            window_toks = sent_toks[i:i+window_size]\n",
        "            \n",
        "            target = window_toks[window_size//2]\n",
        "            for context in window_toks:\n",
        "                if context == target:\n",
        "                    continue\n",
        "                training_pairs.append((torch.tensor(stoi[target]), torch.tensor(stoi[context])))\n",
        "            \n",
        "    # Initialize the Word2Vec model\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    model = Word2Vec(len(vocab), embedding_dim)\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Define the loss function and optimizer\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), learning_rate)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for target_word, context_word in training_pairs:\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            target_word = target_word.to(device)\n",
        "            context_word = context_word.to(device)\n",
        "            target_emb, context_emb = model(target_word, context_word)\n",
        "            \n",
        "            # Compute the loss\n",
        "            loss = loss_func(target_emb, context_emb)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the model parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Accumulate the loss\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "        # Print the average loss for the epoch\n",
        "        print(f\"Epoch {epoch+1} Loss: {round(total_loss/len(training_pairs), 3)}\")\n",
        "        \n",
        "    # Return the trained Word2Vec model\n",
        "    return model, vocab, stoi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_similar(word, embeds, stoi):\n",
        "    word_embed = embeds[stoi[word]]\n",
        "    best_word = ''\n",
        "    best_score = -1\n",
        "    \n",
        "    for w in stoi.keys():\n",
        "        i = stoi[w]\n",
        "        if w == word:\n",
        "            continue\n",
        "        \n",
        "        # cosine sim\n",
        "        s = np.dot(word_embed, embeds[i]) / (np.linalg.norm(word_embed, 2) * np.linalg.norm(embeds[i], 2))\n",
        "        \n",
        "        if s > best_score:\n",
        "            best_score = s\n",
        "            best_word = w\n",
        "\n",
        "    return best_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0MuMxPBQwgJS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 1.621\n",
            "Epoch 2 Loss: -1.014\n",
            "Epoch 3 Loss: -3.616\n",
            "Epoch 4 Loss: -6.285\n",
            "Epoch 5 Loss: -9.044\n",
            "Epoch 6 Loss: -11.919\n",
            "Epoch 7 Loss: -14.944\n",
            "Epoch 8 Loss: -18.152\n",
            "Epoch 9 Loss: -21.583\n",
            "Epoch 10 Loss: -25.275\n",
            "Epoch 11 Loss: -29.266\n",
            "Epoch 12 Loss: -33.591\n",
            "Epoch 13 Loss: -38.281\n",
            "Epoch 14 Loss: -43.365\n",
            "Epoch 15 Loss: -48.865\n",
            "Epoch 16 Loss: -54.8\n",
            "Epoch 17 Loss: -61.189\n",
            "Epoch 18 Loss: -68.045\n",
            "Epoch 19 Loss: -75.382\n",
            "Epoch 20 Loss: -83.21\n",
            "Epoch 21 Loss: -91.54\n",
            "Epoch 22 Loss: -100.379\n",
            "Epoch 23 Loss: -109.736\n",
            "Epoch 24 Loss: -119.618\n",
            "Epoch 25 Loss: -130.028\n",
            "Epoch 26 Loss: -140.965\n",
            "Epoch 27 Loss: -152.422\n",
            "Epoch 28 Loss: -164.393\n",
            "Epoch 29 Loss: -176.868\n",
            "Epoch 30 Loss: -189.835\n",
            "Epoch 31 Loss: -203.285\n",
            "Epoch 32 Loss: -217.208\n",
            "Epoch 33 Loss: -231.592\n",
            "Epoch 34 Loss: -246.427\n",
            "Epoch 35 Loss: -261.704\n",
            "Epoch 36 Loss: -277.41\n",
            "Epoch 37 Loss: -293.534\n",
            "Epoch 38 Loss: -310.063\n",
            "Epoch 39 Loss: -326.987\n",
            "Epoch 40 Loss: -344.294\n",
            "Epoch 41 Loss: -361.975\n",
            "Epoch 42 Loss: -380.021\n",
            "Epoch 43 Loss: -398.424\n",
            "Epoch 44 Loss: -417.173\n",
            "Epoch 45 Loss: -436.261\n",
            "Epoch 46 Loss: -455.676\n",
            "Epoch 47 Loss: -475.409\n",
            "Epoch 48 Loss: -495.45\n",
            "Epoch 49 Loss: -515.787\n",
            "Epoch 50 Loss: -536.412\n",
            "most similar words to learn: fascinating\n",
            "most similar words to love: bos\n",
            "\n",
            "\n",
            "<: [-7.1691866 -0.9134783 -6.7061033 -6.6341968 -5.5634737 -3.927736\n",
            "  9.545232  -4.0116663 -6.366908  -7.1593323]\n",
            "love: [ 5.840488  -4.4041185 -3.552914  -4.14037    5.7352505 -2.75351\n",
            " -1.7520541 -4.4864273 -4.5654564 -4.003641 ]\n",
            "learning: [-3.3964503  6.510492  -4.067929  -5.5105815  6.798441  -5.3306775\n",
            " -4.034586  -4.024253  -6.044027  -4.6623826]\n",
            "to: [ 6.138819   -0.16390269 -3.8041923  -5.1215296  -5.052871   -4.885986\n",
            " -3.3549492  -4.445549    1.9844538   6.9520183 ]\n",
            "bos: [10.003911  -5.2315826 -6.2637367 -5.8941627 -7.143918  -6.861842\n",
            " -7.6206183 -6.5491495 -6.93239   -6.3519125]\n",
            "fascinating: [-4.827855  -5.6402507 -6.267001  -6.1426263 -5.585223  -0.9657145\n",
            "  5.7370214 -5.1069355  5.447721   4.649558 ]\n",
            "I: [ 6.581666  -5.6589847 -3.4227126 -6.618471  -5.6037416 -4.937031\n",
            " -5.307642  -4.1457644 -5.682461  -1.0843253]\n",
            ">: [-7.063016  -6.732407  -5.799501   9.881966  -6.186247  -4.2854614\n",
            " -2.1850564 -4.744426  -6.929455  -0.8105912]\n",
            "deep: [-3.3349004 -4.0498624 -3.652263  -3.9908977 -1.7548357 -3.5033064\n",
            "  6.6080017 -5.4631753  4.52136   -3.8030224]\n",
            "eos: [-0.33929107  0.06143956  0.16772263 -1.4446018   0.21300524  0.13991557\n",
            "  0.19585778  0.12698819  0.622033   -2.5926294 ]\n",
            "learn: [-2.1131768 -5.3093977 -2.0705864  6.44054    6.688476  -5.7191715\n",
            " -4.1763473 -6.012065  -5.2128944 -4.214638 ]\n",
            "It: [ 5.8860397  1.8928541 -4.22086   -3.8207    -3.8316522 -4.827786\n",
            " -4.2640934 -3.5646513  6.602731  -2.715338 ]\n",
            "is: [ 0.9631984 -3.6804037 -5.318313  -3.9492192  5.479417  -4.662733\n",
            " -4.092452  -3.6616085  5.335019  -3.9109735]\n"
          ]
        }
      ],
      "source": [
        "# Define the main function\n",
        "\n",
        "def main():\n",
        "    # Set hyperparameters\n",
        "    corpus = \"I love to learn deep learning. It is fascinating!\"\n",
        "    window_size = 3\n",
        "    embedding_dim = 10\n",
        "    num_epochs = 50\n",
        "    lr = 0.01\n",
        "\n",
        "    # Load and preprocess the corpus\n",
        "    ### DONE in training function\n",
        "    \n",
        "    # Train the Word2Vec model\n",
        "    model, vocab, stoi = train_word2vec(corpus, window_size, embedding_dim, num_epochs, lr)\n",
        "    embeds = model.in_embed.weight.detach().cpu().numpy()\n",
        "    \n",
        "    # Evaluate the trained model using word similarity or analogy tasks\n",
        "    w = most_similar('deep', embeds, stoi)\n",
        "    print(f'most similar words to learn: {w}')\n",
        "    w = most_similar('love', embeds, stoi)\n",
        "    print(f'most similar words to love: {w}')\n",
        "    \n",
        "    # Print the learned word embeddings\n",
        "    print('\\n')\n",
        "    for w in stoi.keys():\n",
        "        i = stoi[w]\n",
        "        print(f'{w}: {embeds[i]}')\n",
        "    \n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), 'word2vec_model.pt')\n",
        "    \n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
