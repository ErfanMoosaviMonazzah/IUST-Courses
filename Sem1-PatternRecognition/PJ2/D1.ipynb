{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Laterality_edited.csv', index_col=0)\n",
    "data.index = range(data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 20 columns (19 inputs, 1 output)<br>\n",
    "35 samples<br>\n",
    "14 cols, contains categorical data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hipp_Vol_LI</th>\n",
       "      <th>Hipp_FLAIR_LI</th>\n",
       "      <th>Cg_LI</th>\n",
       "      <th>Fx_LI</th>\n",
       "      <th>Hipp_MD_LI</th>\n",
       "      <th>Overall_Laterality_NO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.089070</td>\n",
       "      <td>-0.030609</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.382353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.062391</td>\n",
       "      <td>0.266247</td>\n",
       "      <td>0.045279</td>\n",
       "      <td>0.032205</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.493270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.129227</td>\n",
       "      <td>-0.558875</td>\n",
       "      <td>-0.140300</td>\n",
       "      <td>-0.046900</td>\n",
       "      <td>-0.000765</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.065504</td>\n",
       "      <td>-0.307987</td>\n",
       "      <td>-0.057650</td>\n",
       "      <td>-0.008400</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.015769</td>\n",
       "      <td>-0.071029</td>\n",
       "      <td>-0.030700</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.010313</td>\n",
       "      <td>0.079627</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.106508</td>\n",
       "      <td>0.499853</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hipp_Vol_LI  Hipp_FLAIR_LI      Cg_LI      Fx_LI  Hipp_MD_LI  \\\n",
       "count    34.000000      34.000000  34.000000  34.000000   34.000000   \n",
       "mean     -0.015906      -0.089070  -0.030609   0.015556   -0.000051   \n",
       "std       0.062391       0.266247   0.045279   0.032205    0.000191   \n",
       "min      -0.129227      -0.558875  -0.140300  -0.046900   -0.000765   \n",
       "25%      -0.065504      -0.307987  -0.057650  -0.008400   -0.000136   \n",
       "50%      -0.015769      -0.071029  -0.030700   0.020700   -0.000023   \n",
       "75%       0.010313       0.079627  -0.000875   0.042300    0.000047   \n",
       "max       0.106508       0.499853   0.080900   0.071600    0.000284   \n",
       "\n",
       "       Overall_Laterality_NO  \n",
       "count              34.000000  \n",
       "mean                0.382353  \n",
       "std                 0.493270  \n",
       "min                 0.000000  \n",
       "25%                 0.000000  \n",
       "50%                 0.000000  \n",
       "75%                 1.000000  \n",
       "max                 1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input-Output Split\n",
    "The last column of dataset is the output and the other ones are the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is binary, so we are facing binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not do normalization, since we want to use ada boost with deTree base estimator, I expalined why this is the case in second question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with categorical data\n",
    "since we are going to use deTree as base classifier for ada boost, we don't need to convert categorical data to numerical, but because this task was specified in TA class (I heard so, I'm not sure) we're going to do so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we have two different set of numbers, one reperesnting numerical cols the other for categorical cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [5,7,9,11,13]\n",
    "cat_cols = list(set(range(19)).difference(num_cols))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make an ecoder object wich we will use for one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse= False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the encoder object, here we transformed cat features to one hot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_oh = pd.DataFrame(enc.fit_transform(X.iloc[:, cat_cols]))\n",
    "X_num = X.iloc[:, num_cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now in this cell, we merged the numerical features with one hot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oh = pd.concat([X_num, X_cat_oh], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a list of k for k-fold cross validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_k = [5, 7, 10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here we used sklearn cross_validate function to cross validate ada boost with 10,000 base estimators on our data sets, base estimator would be a deTree and during the evaluation we'll evaluate accuracy, precision and recall and F1. we use multi threaded method to reduce training time. -1 means use all available cpu cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_evals = []\n",
    "for k in list_k:\n",
    "    clf = AdaBoostClassifier(n_estimators=10000)\n",
    "    eval = cross_validate(clf, X_oh, y, cv=k, n_jobs=-1, scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "    list_evals.append(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- k:5 ----\n",
      "Folds acc: [0.85714286 0.85714286 0.85714286 0.71428571 0.83333333]\n",
      "Folds precision: [1.         1.         0.75       1.         0.66666667]\n",
      "Folds recall: [0.5        0.66666667 1.         0.33333333 1.        ]\n",
      "Folds F1: [0.66666667 0.8        0.85714286 0.5        0.8       ]\n",
      "Overal ACC: 0.8238095238095238\n",
      "Overal Precision: 0.8833333333333334\n",
      "Overal Recall: 0.7\n",
      "Overal F1: 0.7247619047619048\n",
      "---- k:7 ----\n",
      "Folds acc: [1.   1.   1.   1.   0.6  0.8  0.75]\n",
      "Folds precision: [1.  1.  1.  1.  0.5 1.  0.5]\n",
      "Folds recall: [1.  1.  1.  1.  0.5 0.5 1. ]\n",
      "Folds F1: [1.         1.         1.         1.         0.5        0.66666667\n",
      " 0.66666667]\n",
      "Overal ACC: 0.8785714285714284\n",
      "Overal Precision: 0.8571428571428571\n",
      "Overal Recall: 0.8571428571428571\n",
      "Overal F1: 0.8333333333333334\n",
      "---- k:10 ----\n",
      "Folds acc: [1.         0.75       1.         1.         1.         0.33333333\n",
      " 0.66666667 0.66666667 0.66666667 1.        ]\n",
      "Folds precision: [1.  1.  1.  1.  1.  0.  0.5 0.  0.5 1. ]\n",
      "Folds recall: [1.  0.5 1.  1.  1.  0.  1.  0.  1.  1. ]\n",
      "Folds F1: [1.         0.66666667 1.         1.         1.         0.\n",
      " 0.66666667 0.         0.66666667 1.        ]\n",
      "Overal ACC: 0.8083333333333332\n",
      "Overal Precision: 0.7\n",
      "Overal Recall: 0.75\n",
      "Overal F1: 0.7\n"
     ]
    }
   ],
   "source": [
    "for eval in list_evals:\n",
    "    k = len(eval['fit_time'])\n",
    "    acc_list = eval['test_accuracy']\n",
    "    pre_list = eval['test_precision']\n",
    "    rec_list = eval['test_recall']\n",
    "    f1_list = eval['test_f1']\n",
    "    print(f'---- k:{k} ----')\n",
    "    print(f'Folds acc: {acc_list}')\n",
    "    print(f'Folds precision: {pre_list}')\n",
    "    print(f'Folds recall: {rec_list}')\n",
    "    print(f'Folds F1: {f1_list}')\n",
    "    print(f'Overal ACC: {np.mean(acc_list)}')\n",
    "    print(f'Overal Precision: {np.mean(pre_list)}')\n",
    "    print(f'Overal Recall: {np.mean(rec_list)}')\n",
    "    print(f'Overal F1: {np.mean(f1_list)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on above results, k=7 had the best accuracy, k=5 had best precision and k=7 had the best recall. for this task we are more interested in recall, because bigger recall means smaller false negative, which means more patients with disease classified correctly. In medical cases, classifiying a healthy person as sick is not as dangerous as classifiy a sick patient as healthy, so recall is more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3eb7beeb1f5818634685e8694558c24f571bc2e8856635424260c9d4d7660dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
