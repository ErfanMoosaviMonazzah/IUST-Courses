{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SfIDt-ZZsIHm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK CUDA was avaialbe.\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from utils import device, get_num_correct\n",
        "from vgg16modified import Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK CUDA was avaialbe.\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print('OK CUDA was avaialbe.')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BrVtDiKMMQoG"
      },
      "outputs": [],
      "source": [
        "# declare the transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # add augmentations\n",
        "        transforms.ColorJitter(brightness=0.25, saturation=0.1),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGmTRbzEsIHr",
        "outputId": "32eca5b0-9f07-4d7a-8c62-f906def18313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Downlaoding training set\n",
            "Files already downloaded and verified\n",
            "- Downloading testing set\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# choose the training and test datasets\n",
        "print(f'- Downlaoding training set')\n",
        "train_set = torchvision.datasets.CIFAR10('ds/train/', train=True, download=True, transform=data_transforms['train'])\n",
        "print(f'- Downloading testing set')\n",
        "test_set = torchvision.datasets.CIFAR10('ds/test/', train=False, download=True, transform=data_transforms['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uoWPNbbVsIH0"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "valid_size = 0.5  # percentage of test_set to be used as validation\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_test = len(test_set)\n",
        "indices = list(range(num_test))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_test))\n",
        "test_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "# prepare the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "valid_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, sampler=valid_sampler, num_workers=1)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, sampler=test_sampler, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIQ8mJNfPSeD",
        "outputId": "6a5b7cc9-0e09-4478-8fc7-a3031b50b38f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/erfan/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/erfan/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "vgg16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umFYw1-cW_jM",
        "outputId": "47cf0700-e5ca-48f6-a124-fc0eb3506904"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Network(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# replace the vgg16 classifier\n",
        "model = Network(vgg16)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezed: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Freezed: ReLU(inplace=True)\n",
            "Freezed: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "Freezed: ReLU(inplace=True)\n",
            "Freezed: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
          ]
        }
      ],
      "source": [
        "# Freeze Layers\n",
        "freeze_features = 26\n",
        "freeze_classifier = 0\n",
        "for layer_num, child in enumerate(model.features.children()):\n",
        "    if layer_num < freeze_features:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad_(False)\n",
        "    else:\n",
        "        print(f'Freezed: {child}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezed: Linear(in_features=512, out_features=256, bias=True)\n",
            "Freezed: ReLU(inplace=True)\n",
            "Freezed: Linear(in_features=256, out_features=128, bias=True)\n",
            "Freezed: ReLU(inplace=True)\n",
            "Freezed: Linear(in_features=128, out_features=10, bias=True)\n"
          ]
        }
      ],
      "source": [
        "for layer_num, child in enumerate(model.classifier.children()):\n",
        "    if layer_num < freeze_classifier:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad_(False)\n",
        "    else:\n",
        "        print(f'Freezed: {child}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUq5gbZRsIH6",
        "outputId": "33d85345-d312-4815-cad4-c033a3c0f0bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [ 1/30]: 100%|██████████| 196/196 [00:29<00:00,  6.75it/s, acc=0.512, loss=1.1]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 1.396535\tAvg validation loss: 0.991712\n",
            "\t\tvalid_loss decreased (inf --> 0.991712)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [ 2/30]: 100%|██████████| 196/196 [00:26<00:00,  7.26it/s, acc=0.635, loss=1.14] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 1.034078\tAvg validation loss: 0.911661\n",
            "\t\tvalid_loss decreased (0.991712 --> 0.911661)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [ 3/30]: 100%|██████████| 196/196 [00:30<00:00,  6.42it/s, acc=0.661, loss=1.02] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.955677\tAvg validation loss: 0.835294\n",
            "\t\tvalid_loss decreased (0.911661 --> 0.835294)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [ 4/30]: 100%|██████████| 196/196 [00:32<00:00,  6.01it/s, acc=0.685, loss=1.08] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.896698\tAvg validation loss: 0.782570\n",
            "\t\tvalid_loss decreased (0.835294 --> 0.782570)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [ 5/30]: 100%|██████████| 196/196 [00:34<00:00,  5.70it/s, acc=0.698, loss=0.911]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.857689\tAvg validation loss: 0.764032\n",
            "\t\tvalid_loss decreased (0.782570 --> 0.764032)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [ 6/30]: 100%|██████████| 196/196 [00:32<00:00,  5.99it/s, acc=0.705, loss=0.727]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.834442\tAvg validation loss: 0.753213\n",
            "\t\tvalid_loss decreased (0.764032 --> 0.753213)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [ 7/30]: 100%|██████████| 196/196 [00:31<00:00,  6.28it/s, acc=0.716, loss=0.894]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.800323\tAvg validation loss: 0.731767\n",
            "\t\tvalid_loss decreased (0.753213 --> 0.731767)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [ 8/30]: 100%|██████████| 196/196 [00:32<00:00,  5.98it/s, acc=0.726, loss=0.766]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.776558\tAvg validation loss: 0.716521\n",
            "\t\tvalid_loss decreased (0.731767 --> 0.716521)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [ 9/30]: 100%|██████████| 196/196 [00:31<00:00,  6.17it/s, acc=0.726, loss=0.742]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.770554\tAvg validation loss: 0.707563\n",
            "\t\tvalid_loss decreased (0.716521 --> 0.707563)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [10/30]: 100%|██████████| 196/196 [00:35<00:00,  5.48it/s, acc=0.736, loss=0.59] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.745659\tAvg validation loss: 0.694599\n",
            "\t\tvalid_loss decreased (0.707563 --> 0.694599)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [11/30]: 100%|██████████| 196/196 [00:31<00:00,  6.29it/s, acc=0.74, loss=0.795] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.731402\tAvg validation loss: 0.678858\n",
            "\t\tvalid_loss decreased (0.694599 --> 0.678858)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [12/30]: 100%|██████████| 196/196 [00:31<00:00,  6.20it/s, acc=0.747, loss=0.713]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.716510\tAvg validation loss: 0.682861\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [13/30]: 100%|██████████| 196/196 [00:30<00:00,  6.36it/s, acc=0.75, loss=0.586] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.704470\tAvg validation loss: 0.668189\n",
            "\t\tvalid_loss decreased (0.678858 --> 0.668189)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [14/30]: 100%|██████████| 196/196 [00:37<00:00,  5.29it/s, acc=0.753, loss=0.782]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.695582\tAvg validation loss: 0.671861\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [15/30]: 100%|██████████| 196/196 [00:32<00:00,  6.08it/s, acc=0.758, loss=0.609]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.681115\tAvg validation loss: 0.661429\n",
            "\t\tvalid_loss decreased (0.668189 --> 0.661429)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [16/30]: 100%|██████████| 196/196 [00:36<00:00,  5.42it/s, acc=0.762, loss=0.487]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.671349\tAvg validation loss: 0.658216\n",
            "\t\tvalid_loss decreased (0.661429 --> 0.658216)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [17/30]: 100%|██████████| 196/196 [00:33<00:00,  5.81it/s, acc=0.764, loss=0.589]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.667157\tAvg validation loss: 0.642292\n",
            "\t\tvalid_loss decreased (0.658216 --> 0.642292)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [18/30]: 100%|██████████| 196/196 [00:30<00:00,  6.41it/s, acc=0.768, loss=0.766]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.651778\tAvg validation loss: 0.638804\n",
            "\t\tvalid_loss decreased (0.642292 --> 0.638804)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [19/30]: 100%|██████████| 196/196 [00:30<00:00,  6.34it/s, acc=0.774, loss=0.613]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.640824\tAvg validation loss: 0.637800\n",
            "\t\tvalid_loss decreased (0.638804 --> 0.637800)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [20/30]: 100%|██████████| 196/196 [00:30<00:00,  6.42it/s, acc=0.777, loss=0.499]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.629878\tAvg validation loss: 0.647888\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [21/30]: 100%|██████████| 196/196 [00:30<00:00,  6.39it/s, acc=0.777, loss=0.662]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.625629\tAvg validation loss: 0.634561\n",
            "\t\tvalid_loss decreased (0.637800 --> 0.634561)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [22/30]: 100%|██████████| 196/196 [00:30<00:00,  6.34it/s, acc=0.782, loss=0.676]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.612757\tAvg validation loss: 0.641938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [23/30]: 100%|██████████| 196/196 [00:29<00:00,  6.60it/s, acc=0.782, loss=0.738]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.605817\tAvg validation loss: 0.648690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [24/30]: 100%|██████████| 196/196 [00:30<00:00,  6.45it/s, acc=0.786, loss=0.703]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.600906\tAvg validation loss: 0.625585\n",
            "\t\tvalid_loss decreased (0.634561 --> 0.625585)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [25/30]: 100%|██████████| 196/196 [00:31<00:00,  6.18it/s, acc=0.79, loss=0.695] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.590871\tAvg validation loss: 0.626316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [26/30]: 100%|██████████| 196/196 [00:27<00:00,  7.07it/s, acc=0.79, loss=0.522] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.586805\tAvg validation loss: 0.629201\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [27/30]: 100%|██████████| 196/196 [00:29<00:00,  6.63it/s, acc=0.794, loss=0.535]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.576479\tAvg validation loss: 0.629249\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [28/30]: 100%|██████████| 196/196 [00:31<00:00,  6.26it/s, acc=0.798, loss=0.571]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.564457\tAvg validation loss: 0.624567\n",
            "\t\tvalid_loss decreased (0.625585 --> 0.624567)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [29/30]: 100%|██████████| 196/196 [00:31<00:00,  6.29it/s, acc=0.799, loss=0.497]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.562412\tAvg validation loss: 0.618387\n",
            "\t\tvalid_loss decreased (0.624567 --> 0.618387)  saving model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [30/30]: 100%|██████████| 196/196 [00:29<00:00,  6.56it/s, acc=0.8, loss=0.458]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tAvg training loss: 0.554520\tAvg validation loss: 0.642175\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # loss function (categorical cross-entropy)\n",
        "optimizer = optim.SGD(\n",
        "    [      # parameters which need optimization\n",
        "        {'params':model.features[19:].parameters(), 'lr':0.001},\n",
        "        {'params':model.classifier.parameters()}\n",
        "    ], lr=0.01, momentum=0.9, weight_decay=1e-3)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=1/3, patience=5, verbose=True) # lr scheduler\n",
        "\n",
        "comment = f'-transferlr_vgg16'  # will be used for naming the run\n",
        "tb = SummaryWriter(comment=comment)\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial minimum to infinity\n",
        "num_epochs = 30  # number of epochs used for training\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_correct = 0, 0  # wil be used to track the running loss and correct\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    train_loop = tqdm(train_loader)\n",
        "    model.train()  # set the model to train mode\n",
        "    for batch in train_loop:\n",
        "        images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device (cpu/gpu)\n",
        "        preds = model(images)  # forward pass\n",
        "        loss = criterion(preds, labels)  # calculate loss\n",
        "        optimizer.zero_grad()  # clear accumulated gradients from the previous pass\n",
        "        loss.backward()  # backward pass\n",
        "        optimizer.step()  # perform a single optimization step\n",
        "\n",
        "        train_loss += loss.item() * labels.size(0) # update the running loss\n",
        "        train_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "\n",
        "        train_loop.set_description(f'Epoch [{epoch+1:2d}/{num_epochs}]')\n",
        "        train_loop.set_postfix(loss=loss.item(), acc=train_correct/len(train_set))\n",
        "\n",
        "    # add train loss and train accuracy for the current epoch to tensorboard\n",
        "    tb.add_scalar('Train Loss', train_loss, epoch)\n",
        "    tb.add_scalar('Train Accuracy', train_correct/len(train_set), epoch)\n",
        "\n",
        "    model.eval()  # set the model to evaluation mode\n",
        "    with torch.no_grad():  # turn off grad tracking, as we don't need gradients for validation\n",
        "        valid_loss, valid_correct = 0, 0  # will be used to track the running validation loss and correct\n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "        for batch in valid_loader:\n",
        "            images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device\n",
        "            preds = model(images)  # forward pass\n",
        "            loss = criterion(preds, labels)  # calculate the loss\n",
        "\n",
        "            valid_loss += loss.item() * labels.size(0)  # update the running loss\n",
        "            valid_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "            \n",
        "\n",
        "        # add validation loss and validation accuracy for the current epoch to tensorboard\n",
        "        tb.add_scalar('Validation Loss', valid_loss, epoch)\n",
        "        tb.add_scalar('Validation Accuracy', valid_correct/len(valid_loader.sampler), epoch)\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = train_loss/len(train_set)\n",
        "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "        train_loop.write(f'\\t\\tAvg training loss: {train_loss:.6f}\\tAvg validation loss: {valid_loss:.6f}')\n",
        "        scheduler.step(valid_loss)\n",
        "\n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            train_loop.write(f'\\t\\tvalid_loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f})  saving model...')\n",
        "            torch.save(model.state_dict(), f'./models/model{comment}.pth')\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "\n",
        "        test_loss, test_correct = 0, 0  # will be used to track the running test loss and correct\n",
        "        ##################\n",
        "        # test the model #\n",
        "        ##################\n",
        "        for batch in test_loader:\n",
        "            images, labels = batch[0].to(device), batch[1].to(device)  # load the batch to the available device\n",
        "            preds = model(images)  # forward pass\n",
        "            loss = criterion(preds, labels)  # calculate the loss\n",
        "\n",
        "            test_loss += loss.item() * labels.size(0)  # update the running loss\n",
        "            test_correct += get_num_correct(preds, labels)  # update running num correct\n",
        "\n",
        "        # add test loss and test accuracy for the current epoch to tensorboard\n",
        "        tb.add_scalar('Test Loss', test_loss, epoch)\n",
        "        tb.add_scalar('Test Accuracy', test_correct/len(test_loader.sampler), epoch)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_with_vgg16.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "ce368be3b2dfd6e91cae61dfccf2b578c81b57fad3a95bcdefa081d05d497098"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
