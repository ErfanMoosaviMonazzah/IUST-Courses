{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwtJK7ZeqFxJ",
        "outputId": "33f3db3d-0ba6-464f-87ac-6b6eb160bada"
      },
      "outputs": [],
      "source": [
        "!gdown \"1--2MHS70-Y8rZfsDd5PjztPevBC6Q5vd&confirm=t\"\n",
        "!unzip flickr8kimagescaptions.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cCOiRSc4CZ6o"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h28fn60Ce3r",
        "outputId": "17eaf5b0-10ed-4b25-ed7d-d2a999514449"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import torch \n",
        "import random\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from textwrap import wrap\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "from torchtext.data import get_tokenizer\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import spacy\n",
        "spacy_eng = get_tokenizer('spacy')\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_mM3-q5wCTOE"
      },
      "source": [
        "# Visualize sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcOeOHe8z7lu"
      },
      "outputs": [],
      "source": [
        "captions = open(\"/content/flickr8k/captions.txt\")\n",
        "captions = [line.strip().split(\",\") for line in captions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "-rTrsWnfCS1J",
        "outputId": "261df82c-4879-4151-a6b2-07e4d9b585cd"
      },
      "outputs": [],
      "source": [
        "captions = open(\"/content/flickr8k/captions.txt\")\n",
        "\n",
        "captions = [line.strip().split(\",\") for line in captions]\n",
        "\n",
        "dict_image_caption = {}\n",
        "\n",
        "for i, item in enumerate(captions):\n",
        "    # item 0 is for description of columns and it has not been used\n",
        "    if i==0:\n",
        "        continue\n",
        "    # adding captions to image in dictionary\n",
        "    if item[0] in dict_image_caption.keys():\n",
        "        dict_image_caption[item[0]].append(item[1])\n",
        "    else:\n",
        "        dict_image_caption[item[0]]= [item[1]]\n",
        "\n",
        "# choosing 1 image\n",
        "keys = random.sample(list(dict_image_caption), 1)\n",
        "\n",
        "for key in keys:\n",
        "    img = cv2.imread(os.path.join(\"/content/flickr8k/images\", key))\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    for i,caption in enumerate(dict_image_caption[key]):\n",
        "        ax = plt.subplot(1, len( dict_image_caption[key]), i + 1)\n",
        "        caption = \"\\n\".join(wrap(caption, 15))\n",
        "        plt.imshow(img)\n",
        "        plt.title(caption)\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H6P_CFbSRqRO"
      },
      "source": [
        "# Customize dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "6b-ROfsQp-uc",
        "outputId": "1e96fd95-ec64-4803-acf4-38bfef5b169d"
      },
      "outputs": [],
      "source": [
        "def collator(batch):\n",
        "    ls_ims = [i for i, c in batch]\n",
        "    ls_caps = [c for i, c in batch]\n",
        "    padded_captions = pad_sequence(ls_caps, batch_first=False, padding_value=0)\n",
        "    return (torch.stack(ls_ims), padded_captions)\n",
        "\n",
        "assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mri4aGvyjfn"
      },
      "outputs": [],
      "source": [
        "class Vocab():\n",
        "    def __init__(self):\n",
        "        self.i2s = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "        self.s2i = {item:index for index, item in enumerate(self.i2s)}\n",
        "    \n",
        "    def build_vocabulary(self, caps):\n",
        "        for cap in caps:\n",
        "            for tok in self.tokenize(cap):                \n",
        "                if tok not in self.s2i:\n",
        "                    self.i2s.append(tok)\n",
        "                    self.s2i[tok] = len(self.i2s) - 1\n",
        "    \n",
        "    def tokenize(self, cap):\n",
        "        return [tok for tok in spacy_eng(cap.replace(\"'\",'').replace('\"','').lower())]\n",
        "\n",
        "    def vectorize(self, cap):\n",
        "        tokenized_cap = self.tokenize(cap)\n",
        "        return [self.s2i[tok] if tok in self.s2i else self.s2i[\"<UNK>\"] for tok in tokenized_cap]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.i2s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OcQVLe0x_q0"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, dir_img, dir_ann):\n",
        "        super(MyDataset, self).__init__()\n",
        "        self.dir_img, self.dir_ann = dir_img, dir_ann\n",
        "        self.df = pd.read_csv(dir_ann)\n",
        "        \n",
        "        self.caps = self.df['caption'].tolist()\n",
        "        self.imgs = self.df['image'].tolist()\n",
        "        \n",
        "        self.vocab = Vocab()\n",
        "        self.vocab.build_vocabulary(self.caps)\n",
        "        self.vec_caps = [torch.tensor([self.vocab.s2i['<SOS>']] + self.vocab.vectorize(cap) + [self.vocab.s2i['<EOS>']]) for cap in self.caps]\n",
        "\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        cap = self.vec_caps[index]\n",
        "        img = self.transform(cv2.imread(os.path.join(self.dir_img, self.imgs[index])))\n",
        "        return (img, cap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj_TRHvv_cF8"
      },
      "outputs": [],
      "source": [
        "dataset = MyDataset('content/flickr8k/images', 'content/flickr8k/captions.txt')\n",
        "PAD_VALUE = dataset.vocab.s2i['<PAD>']\n",
        "\n",
        "train_dataset, valid_dataset = random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset)) ])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collator)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, collate_fn=collator)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8BoB3eRWRgJC"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_GfWSwPBFhU"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioning(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(ImageCaptioning, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        # backbone for feature exctract\n",
        "        self.featuresCNN = models.resnet50(pretrained=True)\n",
        "        # convert features to feature vector\n",
        "        for param in self.featuresCNN.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.featuresCNN.fc = nn.Linear(self.featuresCNN.fc.in_features, embed_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        #activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        #RNN\n",
        "        self.embed = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=PAD_VALUE)\n",
        "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size, self.num_layers)\n",
        "        self.linear = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        \n",
        "    def forward(self, images, captions):\n",
        "        features = self.featuresCNN(images)\n",
        "        features = self.relu(features)\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.fc(hiddens)\n",
        "        return outputs\n",
        "    \n",
        "    def generate(self, img, i2s, max = 40):\n",
        "        with torch.no_grad():\n",
        "            gen_toks = []\n",
        "            img = torch.unsqueeze(img, 0)\n",
        "            x = self.relu(self.featuresCNN(img)).unsqueeze(0)\n",
        "            s = None\n",
        "            for _ in range(max):\n",
        "                h, s = self.lstm(x, s)\n",
        "                o = self.fc(h.squeeze(0))\n",
        "                p = o.argmax(1)\n",
        "                gen_toks.append(p.item())\n",
        "                x = self.embed(p).unsqueeze(0)\n",
        "\n",
        "                if i2s[p.item()] == '<EOS>':\n",
        "                    return [i2s[i] for i in gen_toks]\n",
        "    \n",
        "    def bleu_score(self, dataset):\n",
        "        ls_r, ls_h = [], []\n",
        "        pbar = tqdm(dict_image_caption.items(), total = len(dict_image_caption))\n",
        "        for img, caps in pbar:\n",
        "            image = dataset.transform(cv2.imread(os.path.join(dataset.dir_img, img))).to(device)\n",
        "            generated_caption = [token for token in self.generate(image, dataset.vocab.i2s) if token not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']]\n",
        "            \n",
        "            ls_r.append([dataset.vocab.tokenize(c) for c in caps])\n",
        "            ls_h.append(generated_caption)\n",
        "            pbar.set_description(f'Generating Captions for {img}')\n",
        "\n",
        "        print(\"BLEU 1 =\", corpus_bleu(ls_r, ls_h, weights = (1,0,0,0)))   \n",
        "        print(\"BLEU 2 =\", corpus_bleu(ls_r, ls_h, weights = (0.5,0.5,0,0)))   \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s-_8Vt-fRoHB"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rffX0MAJBUHU"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(dataset.vocab.i2s)\n",
        "num_epochs = 10\n",
        "embed_size = 200\n",
        "hidden_size = 200\n",
        "\n",
        "model = ImageCaptioning(vocab_size, embed_size, hidden_size, 3).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "checkpoint = torch.load('model-emb-freeze.pt')\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "    \n",
        "#     cumm_train_loss = 0\n",
        "#     pbar = tqdm(enumerate(train_loader), total = len(train_loader))\n",
        "    \n",
        "#     for i, (imgs, caps) in pbar:\n",
        "#         optimizer.zero_grad()\n",
        "#         imgs, caps = imgs.to(device), caps.to(device)\n",
        "        \n",
        "#         y_hat = model(imgs, caps[:-1])\n",
        "#         loss = criterion(y_hat.reshape(-1, y_hat.shape[2]), caps.reshape(-1))\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "        \n",
        "#         cumm_train_loss += loss.item()\n",
        "#         pbar.set_description(f'Train Epoch {epoch} | loss: {cumm_train_loss / (i+1)}')\n",
        "        \n",
        "#     model.eval()\n",
        "    \n",
        "#     cumm_loss_valid = 0\n",
        "#     pbar = tqdm(enumerate(valid_loader), total = len(valid_loader))\n",
        "    \n",
        "#     with torch.no_grad():\n",
        "#         for i, (imgs, caps) in pbar:\n",
        "#             imgs, caps = imgs.to(device), caps.to(device)\n",
        "            \n",
        "#             y_hat = model(imgs, caps[:-1])\n",
        "#             loss = criterion(y_hat.reshape(-1, y_hat.shape[2]), caps.reshape(-1))\n",
        "            \n",
        "#             cumm_loss_valid += loss.item()\n",
        "#             pbar.set_description(f'Validation | loss: {cumm_loss_valid / (i+1)}')\n",
        "\n",
        "# torch.save(model.state_dict(), 'model-emb-freeze.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHXlXsEgSgBa"
      },
      "outputs": [],
      "source": [
        "model.bleu_score(dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4zfHYMVcUP5J"
      },
      "source": [
        "# Change Model (Glove embedding + Froze ResNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT930Ci1SE7y",
        "outputId": "0d669657-a181-4aaa-e016-4abe1843d2a7"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!ls -lat\n",
        "locale.getpreferredencoding = lambda: 'UTF-8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GabrkAB0SOk_",
        "outputId": "d415cd4f-4dad-458b-cf88-825cbfcc6497"
      },
      "outputs": [],
      "source": [
        "gvocabs, gembeddings = [],[]\n",
        "with open('glove.6B.200d.txt','r', encoding=\"utf-8\") as file:\n",
        "    gloves = file.read().strip().split('\\n')\n",
        "\n",
        "for row in tqdm(gloves):\n",
        "    vals = row.split(' ')\n",
        "    gvocabs.append(vals[0])\n",
        "    gembeddings.append([float(val) for val in vals[1:]])\n",
        "del gloves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyGGNESwT4dW"
      },
      "outputs": [],
      "source": [
        "avocabs = np.array(['<PAD>','<SOS>','<EOS>','<UNK>'] + gvocabs)\n",
        "aembeddings = np.array(gembeddings)\n",
        "emb_pad = np.zeros((1, aembeddings.shape[1])) # pad should be zero\n",
        "emb_sos = np.random.rand(1, aembeddings.shape[1]) # we learn the relation, no need to do something fancy\n",
        "emb_eos = np.random.rand(1, aembeddings.shape[1]) # we learn the relation, no need to do something fancy\n",
        "emb_unk = np.mean(aembeddings, axis=0,keepdims=True) # this is suggested by glove paper writer\n",
        "aembeddings = np.vstack([emb_pad, emb_sos, emb_eos, emb_unk, aembeddings])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9KcXKfZdKdx"
      },
      "outputs": [],
      "source": [
        "class VocabGlove:\n",
        "    def __init__(self):\n",
        "        self.i2s = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "        self.s2i = {item:index for index, item in enumerate(self.i2s)}\n",
        "        self.u = set(self.i2s)\n",
        "\n",
        "    \n",
        "    def build_vocabulary(self, caps):\n",
        "        for cap in tqdm(caps, total=len(caps)):\n",
        "            for tok in self.tokenize(cap):                \n",
        "                if tok not in self.s2i:\n",
        "                    self.i2s.append(tok)\n",
        "                    self.s2i[tok] = len(self.i2s) - 1\n",
        "    \n",
        "    def tokenize(self, cap):\n",
        "        return [tok for tok in spacy_eng(cap.replace(\"'\",'').replace('\"','').lower())]\n",
        "    \n",
        "    def vectorize(self, cap):\n",
        "        tokenized_cap = self.tokenize(cap)\n",
        "        return [self.s2i[tok] if tok in self.s2i else self.s2i[\"<UNK>\"] for tok in tokenized_cap]\n",
        "    \n",
        "    def populate_u(self, caps):\n",
        "        for cap in tqdm(caps, total=len(caps)):\n",
        "            for word in self.tokenize(cap):                \n",
        "                if word in avocabs and word not in self.u:\n",
        "                    self.u.add(word)\n",
        "\n",
        "    def reset_vocabs(self, rvocabs):\n",
        "        self.i2s, self.s2i = [], {}\n",
        "        for i in range(rvocabs.shape[0]):\n",
        "            self.i2s.append(rvocabs[i])\n",
        "            self.s2i[rvocabs[i]] = len(self.i2s) - 1\n",
        "                    \n",
        "    def __len__(self):\n",
        "        return len(self.i2s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I4AE2Tkfi2V"
      },
      "outputs": [],
      "source": [
        "class MyDataset_Glove(Dataset):\n",
        "    def __init__(self, dir_img, dir_ann):\n",
        "        super(MyDataset_Glove, self).__init__()\n",
        "        self.dir_img, self.dir_ann = dir_img, dir_ann\n",
        "        self.df = pd.read_csv(dir_ann)\n",
        "        \n",
        "        self.caps = self.df['caption'].tolist()\n",
        "        self.imgs = self.df['image'].tolist()\n",
        "        \n",
        "        self.vocab = VocabGlove()\n",
        "        self.vocab.populate_u(self.caps)\n",
        "        self.vocab.build_vocabulary(self.caps)\n",
        "        \n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        cap = self.vec_caps[index]\n",
        "        img = self.transform(cv2.imread(os.path.join(self.dir_img, self.imgs[index])))\n",
        "        return (img, cap)\n",
        "\n",
        "    def build_vectorized_captions(self,):\n",
        "        self.vec_caps = [torch.tensor([self.vocab.s2i['<SOS>']] + self.vocab.vectorize(cap) + [self.vocab.s2i['<EOS>']]) for cap in self.caps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "066RFhdAfj_e",
        "outputId": "edd13036-946b-4242-b9cc-d7b7deb047f4"
      },
      "outputs": [],
      "source": [
        "dataset_glove = MyDataset_Glove('/content/flickr8k/images', '/content/flickr8k/captions.txt')\n",
        "todel = [i for i in range(avocabs.shape[0]) if avocabs[i] not in dataset_glove.vocab.s2i]\n",
        "\n",
        "rvocaba = np.delete(avocabs, todel, axis=0)\n",
        "rembeddings = np.delete(aembeddings, todel, axis=0)\n",
        "\n",
        "dataset_glove.vocab.reset_vocabs(rvocaba)\n",
        "dataset_glove.build_vectorized_captions()\n",
        "\n",
        "train_dataset, valid_dataset = random_split(dataset_glove, [int(0.8 * len(dataset_glove)), len(dataset_glove) - int(0.8 * len(dataset_glove))])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collator)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, collate_fn=collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8OjTD7OXxuL"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningGlove(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(ImageCaptioningGlove, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        # backbone for feature exctract\n",
        "        self.featuresCNN = models.resnet50(pretrained=True)\n",
        "        # convert features to feature vector\n",
        "        for param in self.featuresCNN.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.featuresCNN.fc = nn.Linear(self.featuresCNN.fc.in_features, embed_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        #activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        #RNN\n",
        "        # we use glove embeddings instead of training them from scratch\n",
        "        self.embed = torch.nn.Embedding.from_pretrained(torch.from_numpy(rembeddings).float())\n",
        "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size, self.num_layers)\n",
        "        self.linear = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        \n",
        "    def forward(self, images, captions):\n",
        "        features = self.featuresCNN(images)\n",
        "        features = self.relu(features)\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.fc(hiddens)\n",
        "        return outputs\n",
        "    \n",
        "    def generate(self, img, i2s, max = 40):\n",
        "        with torch.no_grad():\n",
        "            gen_toks = []\n",
        "            img = torch.unsqueeze(img, 0)\n",
        "            x = self.relu(self.featuresCNN(img)).unsqueeze(0)\n",
        "            s = None\n",
        "            for _ in range(max):\n",
        "                h, s = self.lstm(x, s)\n",
        "                o = self.fc(h.squeeze(0))\n",
        "                p = o.argmax(1)\n",
        "                gen_toks.append(p.item())\n",
        "                x = self.embed(p).unsqueeze(0)\n",
        "\n",
        "                if i2s[p.item()] == '<EOS>':\n",
        "                    return [i2s[i] for i in gen_toks]\n",
        "    \n",
        "    def bleu_score(self, dataset):\n",
        "        ls_r, ls_h = [], []\n",
        "        pbar = tqdm(dict_image_caption.items(), total = len(dict_image_caption))\n",
        "        for img, caps in pbar:\n",
        "            image = dataset.transform(cv2.imread(os.path.join(dataset.dir_img, img))).to(device)\n",
        "            generated_caption = [token for token in self.generate(image, dataset.vocab.i2s) if token not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']]\n",
        "            \n",
        "            ls_r.append([dataset.vocab.tokenize(c) for c in caps])\n",
        "            ls_h.append(generated_caption)\n",
        "            pbar.set_description(f'Generating Captions for {img}')\n",
        "\n",
        "        print(\"BLEU 1 =\", corpus_bleu(ls_r, ls_h, weights = (1,0,0,0)))   \n",
        "        print(\"BLEU 2 =\", corpus_bleu(ls_r, ls_h, weights = (0.5,0.5,0,0)))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4voAbD3ltF3l"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(dataset.vocab.i2s)\n",
        "num_epochs = 10\n",
        "embed_size = 200\n",
        "hidden_size = 200\n",
        "\n",
        "model = ImageCaptioningGlove(vocab_size, embed_size, hidden_size, 3).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "checkpoint = torch.load('model-glove-freeze.pt')\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "    \n",
        "#     cumm_train_loss = 0\n",
        "#     pbar = tqdm(enumerate(train_loader), total = len(train_loader))\n",
        "    \n",
        "#     for i, (imgs, caps) in pbar:\n",
        "#         optimizer.zero_grad()\n",
        "#         imgs, caps = imgs.to(device), caps.to(device)\n",
        "        \n",
        "#         y_hat = model(imgs, caps[:-1])\n",
        "#         loss = criterion(y_hat.reshape(-1, y_hat.shape[2]), caps.reshape(-1))\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "        \n",
        "#         cumm_train_loss += loss.item()\n",
        "#         pbar.set_description(f'Train Epoch {epoch} | loss: {cumm_train_loss / (i+1)}')\n",
        "        \n",
        "#     model.eval()\n",
        "    \n",
        "#     cumm_loss_valid = 0\n",
        "#     pbar = tqdm(enumerate(valid_loader), total = len(valid_loader))\n",
        "    \n",
        "#     with torch.no_grad():\n",
        "#         for i, (imgs, caps) in pbar:\n",
        "#             imgs, caps = imgs.to(device), caps.to(device)\n",
        "            \n",
        "#             y_hat = model(imgs, caps[:-1])\n",
        "#             loss = criterion(y_hat.reshape(-1, y_hat.shape[2]), caps.reshape(-1))\n",
        "            \n",
        "#             cumm_loss_valid += loss.item()\n",
        "#             pbar.set_description(f'Validation | loss: {cumm_loss_valid / (i+1)}')\n",
        "\n",
        "# torch.save(model.state_dict(), 'model-emb-freeze.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14L59Aj5HTt5"
      },
      "outputs": [],
      "source": [
        "model.bleu_score(dataset_glove)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EuPd_zlBUWZH"
      },
      "source": [
        "# Train Again (Glove embedding + Unfroze ResNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHdgfPh8UVtw"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningGloveUnfreeze(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(ImageCaptioningGloveUnfreeze, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        # backbone for feature exctract\n",
        "        self.featuresCNN = models.resnet50(pretrained=True)\n",
        "        # convert features to feature vector\n",
        "        for param in self.featuresCNN.parameters():\n",
        "            param.requires_grad = True\n",
        "        self.featuresCNN.fc = nn.Linear(self.featuresCNN.fc.in_features, embed_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        #activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        #RNN\n",
        "        self.embed = torch.nn.Embedding.from_pretrained(torch.from_numpy(rembeddings).float())\n",
        "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size, self.num_layers)\n",
        "        self.linear = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        \n",
        "    def forward(self, images, captions):\n",
        "        features = self.featuresCNN(images)\n",
        "        features = self.relu(features)\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.fc(hiddens)\n",
        "        return outputs\n",
        "    \n",
        "    def generate(self, img, i2s, max = 40):\n",
        "        with torch.no_grad():\n",
        "            gen_toks = []\n",
        "            img = torch.unsqueeze(img, 0)\n",
        "            x = self.relu(self.featuresCNN(img)).unsqueeze(0)\n",
        "            s = None\n",
        "            for _ in range(max):\n",
        "                h, s = self.lstm(x, s)\n",
        "                o = self.fc(h.squeeze(0))\n",
        "                p = o.argmax(1)\n",
        "                gen_toks.append(p.item())\n",
        "                x = self.embed(p).unsqueeze(0)\n",
        "\n",
        "                if i2s[p.item()] == '<EOS>':\n",
        "                    return [i2s[i] for i in gen_toks]\n",
        "    \n",
        "    def bleu_score(self, dataset):\n",
        "        ls_r, ls_h = [], []\n",
        "        pbar = tqdm(dict_image_caption.items(), total = len(dict_image_caption))\n",
        "        for img, caps in pbar:\n",
        "            image = dataset.transform(cv2.imread(os.path.join(dataset.dir_img, img))).to(device)\n",
        "            generated_caption = [token for token in self.generate(image, dataset.vocab.i2s) if token not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']]\n",
        "            \n",
        "            ls_r.append([dataset.vocab.tokenize(c) for c in caps])\n",
        "            ls_h.append(generated_caption)\n",
        "            pbar.set_description(f'Generating Captions for {img}')\n",
        "\n",
        "        print(\"BLEU 1 =\", corpus_bleu(ls_r, ls_h, weights = (1,0,0,0)))   \n",
        "        print(\"BLEU 2 =\", corpus_bleu(ls_r, ls_h, weights = (0.5,0.5,0,0)))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uu3hvQnt3e2"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(dataset.vocab.i2s)\n",
        "num_epochs = 10\n",
        "embed_size = 200\n",
        "hidden_size = 200\n",
        "\n",
        "model = ImageCaptioningGloveUnfreeze(vocab_size, embed_size, hidden_size, 3).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "checkpoint = torch.load('model-glove-unfreeze.pt')\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "    \n",
        "#     cumm_train_loss = 0\n",
        "#     pbar = tqdm(enumerate(train_loader), total = len(train_loader))\n",
        "    \n",
        "#     for i, (imgs, caps) in pbar:\n",
        "#         optimizer.zero_grad()\n",
        "#         imgs, caps = imgs.to(device), caps.to(device)\n",
        "        \n",
        "#         y_hat = model(imgs, caps[:-1])\n",
        "#         loss = criterion(y_hat.reshape(-1, y_hat.shape[2]), caps.reshape(-1))\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "        \n",
        "#         cumm_train_loss += loss.item()\n",
        "#         pbar.set_description(f'Train Epoch {epoch} | loss: {cumm_train_loss / (i+1)}')\n",
        "        \n",
        "#     model.eval()\n",
        "    \n",
        "#     cumm_loss_valid = 0\n",
        "#     pbar = tqdm(enumerate(valid_loader), total = len(valid_loader))\n",
        "    \n",
        "#     with torch.no_grad():\n",
        "#         for i, (imgs, caps) in pbar:\n",
        "#             imgs, caps = imgs.to(device), caps.to(device)\n",
        "            \n",
        "#             y_hat = model(imgs, caps[:-1])\n",
        "#             loss = criterion(y_hat.reshape(-1, y_hat.shape[2]), caps.reshape(-1))\n",
        "            \n",
        "#             cumm_loss_valid += loss.item()\n",
        "#             pbar.set_description(f'Validation | loss: {cumm_loss_valid / (i+1)}')\n",
        "\n",
        "# torch.save(model.state_dict(), 'model-emb-freeze.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlMSjYwgKVQZ"
      },
      "outputs": [],
      "source": [
        "model.bleu_score(dataset_glove)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oedvXgmkv7UG",
        "cCOiRSc4CZ6o",
        "_mM3-q5wCTOE",
        "H6P_CFbSRqRO",
        "8BoB3eRWRgJC",
        "EuPd_zlBUWZH"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
